{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install bert_score\n",
        "# !pip install evaluate"
      ],
      "metadata": {
        "id": "b9-Zt_mfGuBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers"
      ],
      "metadata": {
        "id": "LGTjIywtG5tB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASxSl5mZGsKg",
        "outputId": "eb44d461-830f-4dcb-b762-86fff3c5bc66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 26%|██▌       | 9/35 [02:57<08:54, 20.54s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            " 37%|███▋      | 13/35 [04:24<07:26, 20.28s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            " 46%|████▌     | 16/35 [05:04<05:00, 15.82s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "100%|██████████| 35/35 [11:53<00:00, 20.38s/it]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERTScore: 0.8711325526237488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
            "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForQuestionAnswering\n",
        "from bert_score import score\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate_bertscore(model_path, data_path):\n",
        "    model = BertForQuestionAnswering.from_pretrained(model_path)\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        squad_data = json.load(f)[\"data\"]\n",
        "\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "\n",
        "    for article in tqdm(squad_data):\n",
        "        for paragraph in article[\"paragraphs\"]:\n",
        "            context = paragraph[\"context\"]\n",
        "\n",
        "            for qa in paragraph[\"qas\"]:\n",
        "                question = qa[\"question\"]\n",
        "\n",
        "                if len(qa[\"answers\"]) == 0:\n",
        "                    continue\n",
        "\n",
        "                reference = qa[\"answers\"][0][\"text\"]\n",
        "\n",
        "                encoded_inputs = tokenizer.encode_plus(question, context, padding=\"max_length\", truncation=True,\n",
        "                                                       max_length=512, return_tensors=\"pt\")\n",
        "                input_ids = encoded_inputs[\"input_ids\"].to(device)\n",
        "                attention_mask = encoded_inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                    start_scores = outputs.start_logits\n",
        "                    end_scores = outputs.end_logits\n",
        "\n",
        "                # Convert the predicted answer indices to tokens\n",
        "                all_tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\n",
        "                answer = \" \".join(all_tokens[torch.argmax(start_scores): torch.argmax(end_scores) + 1])\n",
        "                answer = answer.replace(\"[CLS]\", \"\").replace(\"[SEP]\", \"\").replace(\" ##\", \"\")\n",
        "\n",
        "                references.append(reference)\n",
        "                hypotheses.append(answer)\n",
        "\n",
        "    _, _, bert_scores = score(hypotheses, references, lang=\"en\", model_type=\"bert-base-uncased\")\n",
        "\n",
        "    return bert_scores.mean().item()\n",
        "\n",
        "model_path = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
        "data_path = \"dev-v2.0.json\"\n",
        "\n",
        "bert_score = evaluate_bertscore(model_path, data_path)\n",
        "print(\"BERTScore:\", bert_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M7Gb0ueAGtfz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}